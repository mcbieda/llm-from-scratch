{"cells":[{"cell_type":"markdown","id":"b263e8cd","metadata":{"id":"b263e8cd"},"source":["# gpt2_basic_training\n","## 2025DEC08\n"]},{"cell_type":"markdown","source":["# 0. DETECTION OF GOOGLE DRIVE AND SETUP PATHS  \n","Some basics for detection of whether using google drive and colaboratory.  \n","Strong recommend to do this - very easy setup and much faster operation using a GPU (T4 GPU as of January 2026). See README.md for installation instructions.\n","\n","Inspect the various path variables (PROJECT_ROOT, DATA_DIR, OUTPUT_DIR, SRC_DIR) if there are problems."],"metadata":{"id":"bRn67b_2Zn-s"},"id":"bRn67b_2Zn-s"},{"cell_type":"code","source":["# CELL 1: Optional Google Drive mount (Colab only)\n","import os\n","from pathlib import Path\n","\n","def in_colab() -> bool:\n","    return \"COLAB_GPU\" in os.environ or \"COLAB_TPU_ADDR\" in os.environ or \"google.colab\" in str(getattr(__import__(\"sys\"), \"modules\", {}))\n","\n","IN_COLAB = in_colab()\n","\n","if IN_COLAB:\n","    try:\n","        from google.colab import drive  # type: ignore\n","        drive.mount(\"/content/drive\")\n","        print(\"Mounted Google Drive at /content/drive\")\n","    except Exception as e:\n","        print(f\"Could not mount Google Drive: {e}\")\n","else:\n","    print(\"Not running in Colab; skipping Drive mount.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nz13uI7r9clY","executionInfo":{"status":"ok","timestamp":1768362122863,"user_tz":480,"elapsed":1038,"user":{"displayName":"Mark Bie","userId":"10255256717904798861"}},"outputId":"ad3449a7-de2d-45d1-a215-335bf94a1ab0"},"id":"nz13uI7r9clY","execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Mounted Google Drive at /content/drive\n"]}]},{"cell_type":"code","source":["\n","# CELL 2: Resolve PROJECT_ROOT (env override > Colab Drive default > search from cwd)\n","import os\n","import sys\n","from pathlib import Path\n","\n","env_root = os.environ.get(\"LLM_PROJECT_ROOT\", \"\").strip()\n","default_colab_root = Path(\"/content/drive/MyDrive/llm-from-scratch-drive\")\n","\n","def find_repo_root(start: Path) -> Path:\n","    for p in [start, *start.parents]:\n","        if (p / \"src\").exists():\n","            return p\n","    return start\n","\n","if env_root:\n","    PROJECT_ROOT = Path(env_root).expanduser().resolve()\n","elif default_colab_root.exists():\n","    PROJECT_ROOT = default_colab_root.resolve()\n","else:\n","    PROJECT_ROOT = find_repo_root(Path.cwd().resolve())\n","\n","print(\"CWD:\", Path.cwd())\n","print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n","print(\"SRC exists:\", (PROJECT_ROOT / \"src\").exists())\n"],"metadata":{"id":"K6MeMDBc_nk_"},"id":"K6MeMDBc_nk_","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"51f42b58","metadata":{"id":"51f42b58"},"source":["## 1. package loading"]},{"cell_type":"code","execution_count":null,"id":"cd6f3022","metadata":{"id":"cd6f3022"},"outputs":[],"source":["#import os\n","#import sys\n","#from pathlib import Path\n","#import torch\n","#import torch.nn as nn\n","#import tiktoken\n","#from pathlib import Path\n","#import torch\n","#import matplotlib.pyplot as plt\n","#from matplotlib.ticker import MaxNLocator\n","#from datetime import datetime\n","#from torch.utils.data import Dataset, DataLoader\n","#import json\n","#from copy import deepcopy\n","\n","# CELL 3: Imports\n","import json\n","from copy import deepcopy\n","from datetime import datetime\n","\n","import torch\n","import torch.nn as nn\n","import tiktoken\n","\n","import matplotlib.pyplot as plt\n","from matplotlib.ticker import MaxNLocator\n","from torch.utils.data import Dataset, DataLoader\n","\n"]},{"cell_type":"markdown","id":"18185ddd","metadata":{"id":"18185ddd"},"source":[]},{"cell_type":"markdown","id":"6a449902","metadata":{"id":"6a449902"},"source":["## 2. setup paths for llm package load"]},{"cell_type":"code","execution_count":null,"id":"bab9e09e","metadata":{"id":"bab9e09e"},"outputs":[],"source":["\n","# CELL 4: Paths + sys.path + project imports\n","SRC_DIR = PROJECT_ROOT / \"src\"\n","DATA_DIR = PROJECT_ROOT / \"data\"\n","OUTPUT_DIR = PROJECT_ROOT / \"output\"\n","\n","# Create output dir if missing (safe)\n","OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n","\n","print(\"SRC_DIR:\", SRC_DIR)\n","print(\"DATA_DIR:\", DATA_DIR)\n","print(\"OUTPUT_DIR:\", OUTPUT_DIR)\n","\n","# Add src to import path (notebook-friendly)\n","if SRC_DIR.exists() and str(SRC_DIR) not in sys.path:\n","    sys.path.insert(0, str(SRC_DIR))\n","\n","# Now your imports work locally or in Colab\n","from llm_from_scratch.configs import gpt2small_config\n","from llm_from_scratch.training import training_utils\n","from llm_from_scratch.models import gpt2\n","from llm_from_scratch.dataloader import dataloader\n"]},{"cell_type":"markdown","id":"4333239c","metadata":{"id":"4333239c"},"source":["## 3. setup run_training()"]},{"cell_type":"code","execution_count":null,"id":"4457b726","metadata":{"id":"4457b726"},"outputs":[],"source":["def run_training(cfg):\n","    \"\"\"Run a single training experiment given a config dict.\"\"\"\n","    cfg = deepcopy(cfg)  # avoid in-place mutation\n","\n","    model_cfg = cfg['model_config']\n","\n","    # adjust device if cuda not available and cuda was chosen\n","    device = cfg['device_name']\n","    print(\"DEVICE: initial device before adjustment:\",device)\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    print(\"DEVICE: device AFTER adjustment:\",str(device))\n","    cfg['device_name'] = str(device)\n","\n","    torch.manual_seed(cfg['seed'])\n","    print(cfg)\n","    # modify cfg)\n","    # setup model\n","    model = gpt2.setup_model(model_cfg)\n","    model.to(device)\n","    totparams = sum(p.numel() for p in model.parameters())\n","    print(\"Total number of parameters:\", totparams)\n","\n","    # DEBUG: check model\n","    print(\"weight_tying flag:\", model_cfg[\"weight_tying\"])\n","    print(\"same object?:\", model.out_head.weight is model.tok_emb.weight)\n","    print(\"out_head.weight shape:\", model.out_head.weight.shape)\n","    print(\"tok_emb.weight shape:\", model.tok_emb.weight.shape)\n","\n","\n","    # tokenizer\n","    tokenizer = tiktoken.get_encoding(cfg['tokenizer'])\n","\n","    # dataloaders\n","    train_loader, val_loader, test_loader = dataloader.generate_data_loaders(cfg)\n","    print(\"Show train_loader first entry (converted to text):\")\n","    dataloader.loader_text_examine(train_loader, 0, tokenizer)\n","    print(\"Show val_loader first entry (converted to text):\")\n","    dataloader.loader_text_examine(val_loader, 0, tokenizer)\n","    if test_loader is not None:\n","        print(\"Show test_loader first entry (converted to text):\")\n","        dataloader.loader_text_examine(test_loader, 0, tokenizer)\n","\n","    # DEBUG: check\n","    # get one batch\n","    # input_batch, target_batch = next(iter(train_loader))\n","    # input_batch = input_batch.to(device)\n","    # target_batch = target_batch.to(device)\n","\n","    # with torch.no_grad():\n","    #     logits = model(input_batch)\n","\n","    # print(\"tok_emb weight: mean, std:\",\n","    #     model.tok_emb.weight.mean().item(),\n","    #     model.tok_emb.weight.std().item())\n","    # print(\"out_head weight: mean, std:\",\n","    #     model.out_head.weight.mean().item(),\n","    #     model.out_head.weight.std().item())\n","\n","    # print(\"embeds std:\", model.tok_emb(input_batch).std().item())\n","    # print(\"logits: mean, std, min, max:\",\n","    #     logits.mean().item(),\n","    #     logits.std().item(),\n","    #     logits.min().item(),\n","    #     logits.max().item())\n","\n","\n","    # training loop\n","    model.train()\n","    num_epochs = cfg['num_epochs']\n","    optimizer = training_utils.setup_optimizer(model, cfg)\n","    train_losses, val_losses, tokens_seen, global_step = training_utils.train_model_simple(\n","        model=model,\n","        train_loader=train_loader,\n","        val_loader=val_loader,\n","        optimizer=optimizer,\n","        device=cfg['device_name'],\n","        num_epochs=cfg['num_epochs'],\n","        eval_freq=5,\n","        eval_iter=5,\n","        start_context=\"Every effort moves you\",\n","        tokenizer=tokenizer\n","    )\n","\n","    # plot and save plot\n","    epochs_tensor = torch.linspace(0, cfg['num_epochs'], len(train_losses))\n","    training_utils.plot_losses(cfg, epochs_tensor, tokens_seen, train_losses, val_losses)\n","\n","    # save cfg and checkpoint\n","    training_utils.save_cfg_json(\n","        cfg=cfg,\n","        epoch=num_epochs,\n","        global_step=global_step)\n","    # DEBUG: temp disable\n","    # training_utils.save_checkpoint(\n","    #     model=model,\n","    #     optimizer=optimizer,\n","    #     cfg=cfg,\n","    #     epoch=cfg['num_epochs'],\n","    #     global_step=global_step,\n","    # )\n","    training_utils.save_results(cfg,\n","        train_losses, val_losses, tokens_seen, global_step)\n","\n","\n","    # return some results\n","    return {\n","        \"final_train_loss\": float(train_losses[-1]),\n","        \"final_val_loss\": float(val_losses[-1]),\n","        \"tokens_seen\": int(tokens_seen[-1]),\n","        \"global_step\": int(global_step),\n","    }\n","\n"]},{"cell_type":"markdown","id":"24a02590","metadata":{"id":"24a02590"},"source":["## 4. EXPERIMENT 1: basic params"]},{"cell_type":"markdown","id":"b89b07c1","metadata":{"id":"b89b07c1"},"source":["### configs"]},{"cell_type":"code","execution_count":null,"id":"5045ad1e","metadata":{"id":"5045ad1e"},"outputs":[],"source":["cfg = gpt2small_config.RUN_CONFIG\n","cfg\n","cfg2 = deepcopy(cfg)\n","cfg2['run_name'] = \"gpt2_basic_exp_1\"\n","cfg2['description'] = \"see run_name\"\n","cfg2['num_epochs']=6 # just to check things\n","cfg2['device_name'] = \"cuda\"\n","\n","# set training file\n","cfg2['training_file'] = str(DATA_DIR) + \"/the-verdict.txt\"\n","print(cfg2['training_file'])\n","\n","# set output directory\n","cfg2['output_dir'] = str(OUTPUT_DIR)\n","print(cfg2['output_dir'])\n","\n","\n"]},{"cell_type":"markdown","id":"00d92cf5","metadata":{"id":"00d92cf5"},"source":["### run model"]},{"cell_type":"code","execution_count":null,"id":"ade54a77","metadata":{"id":"ade54a77"},"outputs":[],"source":["run_training(cfg2)"]},{"cell_type":"markdown","id":"c906f18b","metadata":{"id":"c906f18b"},"source":["## Experiment 1a: make sure that model really resets"]},{"cell_type":"code","execution_count":null,"id":"7529c5c7","metadata":{"id":"7529c5c7"},"outputs":[],"source":["cfg2['run_name']=\"gpt2_basic_exp_1a\"\n","run_training(cfg2)"]},{"cell_type":"markdown","id":"cbf6b450","metadata":{"id":"cbf6b450"},"source":["## Experiment 1b: try alternate text\n","There are two rationales for this, one for this current work and one looking forward.\n","1. For this current work, it is interesting to see if different corpora perform significantly differently. Both are open source short stories (or part of a short story, truncated to be similar length to the first, for the second). This is just a basic check to see if reasonably similar sources vary much. Note also the concern that the test set is simply the last part of each story; this could easily have different characteristics than the rest of the text. Visual inspection doesn't reveal anything crazy (like if the last part were an author's note with different words and style than earlier) - but this could obviously be quantified by looking at word frequency or ngram frequency, etc. Simply comparing loss curves (both training and validation) for these two stories allows a quick and dirty examination of the effect of source material, when the source material is similar.\n","\n","2. Longer run: rationale for this is that there is a concern that \"the-verdict\" text may have been part of the training set of GPT2. I found an \"open source\" text that is from ~2024/2025, so should not have been part of the ~2019 training of GPT2. This is not an issue here with training from scratch, but for continued pre-training and evaluation, it is. So the test here is to see if this alterate text performs similarly, so I can use it with continued pre-training experiments in the GPT2-small framework.  \n","\n","3. I expect minor some minor differences; these texts are sylistically quite different in many ways, which can affect training.  \n"]},{"cell_type":"code","execution_count":null,"id":"1c3ee1d0","metadata":{"id":"1c3ee1d0"},"outputs":[],"source":["cfg2['run_name']=\"gpt2_basic_exp_1b\"\n","cfg2['description']=\"gpt2 with the adjusted watch story\"\n","# \"training_file\": \"/home/markb/llm-from-scratch/data/the-verdict.txt\"\n","cfg2['training_file'] = PROJECT_ROOT +  \"/data/The-watch-story-adj-smaller-2.txt\"\n","run_training(cfg2)"]},{"cell_type":"markdown","id":"837ec5f1","metadata":{"id":"837ec5f1"},"source":["### RESULTS  \n","1. Curves are quite similar with the two texts.  \n","2. With identical number of tokens seen, have somewhat larger training loss with the watch vs the verdict and slightly higher validation loss.  \n","3. Implications:  \n","    a. these texts do not appear to be some crazy texts that would mess with interpretation in these simple basic frameworks.  \n","    b. these texts can both be used later on for looking at the pre-trained models. The first text may have been part of the training set for the GPT2-small, but the second text is well after the training cutoff, so almost certainly was not."]},{"cell_type":"markdown","id":"b540d142","metadata":{"id":"b540d142"},"source":["## Experiment 2: change stride to do more training"]},{"cell_type":"code","execution_count":null,"id":"3e9752dc","metadata":{"id":"3e9752dc"},"outputs":[],"source":["cfg2['run_name']=\"gpt2_basic_exp_2\"\n","cfg2['stride']=128\n","cfg2"]},{"cell_type":"markdown","id":"5a3f3de1","metadata":{"id":"5a3f3de1"},"source":["### run model"]},{"cell_type":"code","execution_count":null,"id":"3a2039ac","metadata":{"id":"3a2039ac"},"outputs":[],"source":["run_training(cfg2)"]},{"cell_type":"markdown","id":"f34355a1","metadata":{"id":"f34355a1"},"source":["## Experiment 3: make a much smaller model 2 layers, 2 heads, and emb_dim of 256"]},{"cell_type":"code","execution_count":null,"id":"2ede47b4","metadata":{"id":"2ede47b4"},"outputs":[],"source":["cfg2['run_name']=\"gpt2_basic_exp_3\"\n","cfg2['model_config'][\"emb_dim\"]= 256         # Embedding dimension\n","cfg2['model_config'][\"n_heads\"]= 2          # Number of attention heads\n","cfg2['model_config'][\"n_layers\"]= 2          # Number of layers\n","cfg2"]},{"cell_type":"code","execution_count":null,"id":"51bf687e","metadata":{"id":"51bf687e"},"outputs":[],"source":["run_training(cfg2)"]},{"cell_type":"markdown","id":"a7e32207","metadata":{"id":"a7e32207"},"source":["## Experiment 4: do more epochs"]},{"cell_type":"code","execution_count":null,"id":"7edfee44","metadata":{"id":"7edfee44"},"outputs":[],"source":["cfg2['run_name']=\"gpt2_basic_exp_4\"\n","cfg2['num_epochs']=8"]},{"cell_type":"code","execution_count":null,"id":"e402ecaa","metadata":{"id":"e402ecaa"},"outputs":[],"source":["run_training(cfg2)"]},{"cell_type":"markdown","id":"b6f21e1f","metadata":{"id":"b6f21e1f"},"source":["## Experiment 5: mini-model, 2 layers, 4 heads per layer, 6 epochs"]},{"cell_type":"code","execution_count":null,"id":"7e24b3e2","metadata":{"id":"7e24b3e2"},"outputs":[],"source":["cfg2['run_name']=\"gpt2_basic_exp_5\"\n","cfg2['num_epochs']=6\n","cfg2['model_config']['n_layers']=2\n","cfg2['model_config']['n_heads']=4\n","cfg2"]},{"cell_type":"code","execution_count":null,"id":"7a64d796","metadata":{"id":"7a64d796"},"outputs":[],"source":["run_training(cfg2)"]},{"cell_type":"markdown","id":"ec994124","metadata":{"id":"ec994124"},"source":["## Experiment 6: mini-model, 2 layers, 8 heads per layer, epochs = 6"]},{"cell_type":"code","execution_count":null,"id":"a43e1728","metadata":{"id":"a43e1728"},"outputs":[],"source":["cfg2['run_name']=\"gpt2_basic_exp_6\"\n","cfg2['num_epochs']=6\n","cfg2['model_config']['n_layers']=2\n","cfg2['model_config']['n_heads']=8\n","cfg2"]},{"cell_type":"code","execution_count":null,"id":"eed90a17","metadata":{"id":"eed90a17"},"outputs":[],"source":["run_training(cfg2)"]},{"cell_type":"markdown","id":"7e398512","metadata":{"id":"7e398512"},"source":["## Experiment 7: mini-model, 8 layers, 2 heads per layer, 6 epochs"]},{"cell_type":"code","execution_count":null,"id":"1589f49c","metadata":{"id":"1589f49c"},"outputs":[],"source":["cfg2['run_name']=\"gpt2_basic_exp_6\"\n","#cfg2['model_config']['weight_tying']=True\n","cfg2['num_epochs']=6\n","cfg2['model_config']['n_layers']=8\n","cfg2['model_config']['n_heads']=2\n","#cfg2['model_config']['emb_dim']=384\n","cfg2"]},{"cell_type":"code","execution_count":null,"id":"4556c22b","metadata":{"id":"4556c22b"},"outputs":[],"source":["run_training(cfg2)"]},{"cell_type":"markdown","id":"6372fb4b","metadata":{"id":"6372fb4b"},"source":["## Experiment 8: mini-model, 8 layers, 2 heads per layer, emb_dim=64 6 epochs"]},{"cell_type":"code","execution_count":null,"id":"14c83dad","metadata":{"id":"14c83dad"},"outputs":[],"source":["## Experiment 7: mini-model, 8 layers, 2 heads per layer, 6 epochs\n","cfg2['run_name']=\"gpt2_basic_exp_6\"\n","#cfg2['model_config']['weight_tying']=True\n","cfg2['num_epochs']=6\n","cfg2['model_config']['n_layers']=8\n","cfg2['model_config']['n_heads']=2\n","cfg2['model_config']['emb_dim']=64\n","cfg2\n","run_training(cfg2)"]},{"cell_type":"markdown","id":"8eba972f","metadata":{"id":"8eba972f"},"source":[]},{"cell_type":"markdown","id":"78863600","metadata":{"id":"78863600"},"source":["## Experiment 9: mini-model, 8 layers, 2 heads per layer, emb_dim=64 12 epochs"]},{"cell_type":"code","execution_count":null,"id":"99ee7c2c","metadata":{"id":"99ee7c2c"},"outputs":[],"source":["\n","cfg2['run_name']=\"gpt2_basic_exp_9\"\n","#cfg2['model_config']['weight_tying']=True\n","cfg2['num_epochs']=12\n","cfg2['model_config']['n_layers']=8\n","cfg2['model_config']['n_heads']=2\n","cfg2['model_config']['emb_dim']=64\n","cfg2\n","run_training(cfg2)"]},{"cell_type":"markdown","id":"b63ca1ac","metadata":{"id":"b63ca1ac"},"source":["## Experiment 10: mini-model, 8 layers, 8 heads per layer, emb_dim=768 6 epochs"]},{"cell_type":"code","execution_count":null,"id":"e1a5462d","metadata":{"id":"e1a5462d"},"outputs":[],"source":["\n","cfg2['run_name']=\"gpt2_basic_exp_10\"\n","#cfg2['model_config']['weight_tying']=True\n","cfg2['num_epochs']=6\n","cfg2['model_config']['n_layers']=8\n","cfg2['model_config']['n_heads']=8\n","cfg2['model_config']['emb_dim']=768\n","cfg2\n","run_training(cfg2)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"colab":{"provenance":[{"file_id":"1eWfPIVn5ta5264Bw4FDDojMVsxje7DFV","timestamp":1768355448054},{"file_id":"1kf4RtmttXZxIyYoTk47_6ViXRgVPymEf","timestamp":1768267794612}],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}