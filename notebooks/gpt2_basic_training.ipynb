{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b263e8cd",
   "metadata": {},
   "source": [
    "# gpt2_basic_training\n",
    "## 2025DEC08\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f42b58",
   "metadata": {},
   "source": [
    "## 1. package loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd6f3022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18185ddd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a449902",
   "metadata": {},
   "source": [
    "## 2. setup paths for llm package load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bab9e09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/markb/llm-from-scratch/src\n"
     ]
    }
   ],
   "source": [
    "\n",
    "PROJECT_ROOT = Path().resolve().parents[0]          # -> .../project_root\n",
    "SRC_DIR = PROJECT_ROOT / \"src\"           # -> .../project_root/src\n",
    "print(SRC_DIR)\n",
    "\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "from llm_from_scratch.configs import gpt2small_config\n",
    "from llm_from_scratch.training import training_utils\n",
    "from llm_from_scratch.models import gpt2\n",
    "from llm_from_scratch.dataloader import dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4333239c",
   "metadata": {},
   "source": [
    "## 3. setup run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4457b726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(cfg):\n",
    "    \"\"\"Run a single training experiment given a config dict.\"\"\"\n",
    "    cfg = deepcopy(cfg)  # avoid in-place mutation\n",
    "\n",
    "    model_cfg = cfg['model_config']\n",
    "    device = cfg['device_name']\n",
    "    torch.manual_seed(cfg['seed'])\n",
    "    print(cfg)\n",
    "    # modify cfg)\n",
    "    # setup model\n",
    "    model = gpt2.setup_model(model_cfg)\n",
    "    totparams = sum(p.numel() for p in model.parameters())\n",
    "    print(\"Total number of parameters:\", totparams)\n",
    "\n",
    "    # DEBUG: check model\n",
    "    print(\"weight_tying flag:\", model_cfg[\"weight_tying\"])\n",
    "    print(\"same object?:\", model.out_head.weight is model.tok_emb.weight)\n",
    "    print(\"out_head.weight shape:\", model.out_head.weight.shape)\n",
    "    print(\"tok_emb.weight shape:\", model.tok_emb.weight.shape)\n",
    "\n",
    "\n",
    "    # tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(cfg['tokenizer'])\n",
    "\n",
    "    # dataloaders\n",
    "    train_loader, val_loader, test_loader = dataloader.generate_data_loaders(cfg)\n",
    "    print(\"Show train_loader first entry (converted to text):\")\n",
    "    dataloader.loader_text_examine(train_loader, 0, tokenizer)\n",
    "    print(\"Show val_loader first entry (converted to text):\")\n",
    "    dataloader.loader_text_examine(val_loader, 0, tokenizer)\n",
    "    if test_loader is not None:\n",
    "        print(\"Show test_loader first entry (converted to text):\")\n",
    "        dataloader.loader_text_examine(test_loader, 0, tokenizer)\n",
    "    \n",
    "    # DEBUG: check\n",
    "    # get one batch\n",
    "    input_batch, target_batch = next(iter(train_loader))\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_batch)\n",
    "\n",
    "    print(\"tok_emb weight: mean, std:\",\n",
    "        model.tok_emb.weight.mean().item(),\n",
    "        model.tok_emb.weight.std().item())\n",
    "    print(\"out_head weight: mean, std:\",\n",
    "        model.out_head.weight.mean().item(),\n",
    "        model.out_head.weight.std().item())\n",
    "\n",
    "    print(\"embeds std:\", model.tok_emb(input_batch).std().item())\n",
    "    print(\"logits: mean, std, min, max:\",\n",
    "        logits.mean().item(),\n",
    "        logits.std().item(),\n",
    "        logits.min().item(),\n",
    "        logits.max().item())\n",
    "\n",
    "\n",
    "    # training loop\n",
    "    model.train()\n",
    "    num_epochs = cfg['num_epochs']\n",
    "    optimizer = training_utils.setup_optimizer(model, cfg)\n",
    "    train_losses, val_losses, tokens_seen, global_step = training_utils.train_model_simple(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        device=cfg['device_name'],\n",
    "        num_epochs=cfg['num_epochs'],\n",
    "        eval_freq=5,\n",
    "        eval_iter=5,\n",
    "        start_context=\"Every effort moves you\",\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    # plot and save plot\n",
    "    epochs_tensor = torch.linspace(0, cfg['num_epochs'], len(train_losses))\n",
    "    training_utils.plot_losses(cfg, epochs_tensor, tokens_seen, train_losses, val_losses)\n",
    "\n",
    "    # save cfg and checkpoint\n",
    "    training_utils.save_cfg_json(\n",
    "        cfg=cfg, \n",
    "        epoch=num_epochs, \n",
    "        global_step=global_step)  \n",
    "    training_utils.save_checkpoint(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        cfg=cfg,\n",
    "        epoch=cfg['num_epochs'],\n",
    "        global_step=global_step,\n",
    "    )\n",
    "    training_utils.save_results(cfg,\n",
    "        train_losses, val_losses, tokens_seen, global_step)\n",
    "    \n",
    "\n",
    "    # return some results\n",
    "    return {\n",
    "        \"final_train_loss\": float(train_losses[-1]),\n",
    "        \"final_val_loss\": float(val_losses[-1]),\n",
    "        \"tokens_seen\": int(tokens_seen[-1]),\n",
    "        \"global_step\": int(global_step),\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a02590",
   "metadata": {},
   "source": [
    "## 4. EXPERIMENT 1: basic params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89b07c1",
   "metadata": {},
   "source": [
    "### configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5045ad1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = gpt2small_config.RUN_CONFIG\n",
    "cfg\n",
    "cfg2 = deepcopy(cfg)\n",
    "cfg2['run_name'] = \"gpt2_another_basic_exp_1\"\n",
    "cfg2['num_epochs']=8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d92cf5",
   "metadata": {},
   "source": [
    "### run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ade54a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'run_name': 'gpt2_another_basic_exp_1', 'description': 'gpt2-small on the verdict', 'device_name': 'cpu', 'model_name': 'gpt2-small', 'model_config': {'vocab_size': 50257, 'context_length': 256, 'emb_dim': 768, 'n_heads': 12, 'n_layers': 12, 'drop_rate': 0.1, 'qkv_bias': False, 'weight_tying': False}, 'tokenizer': 'gpt2', 'pretrained': False, 'training_file': '/home/markb/llm-from-scratch/data/the-verdict.txt', 'val_file': '', 'test_file': '', 'val_ratio': 0.1, 'test_ratio': 0.0, 'stride': 256, 'batch_size': 2, 'lr': 0.0004, 'weight_decay': 0.1, 'num_epochs': 8, 'seed': 123, 'output_dir': '/home/markb/llm-from-scratch/output'}\n",
      "Total number of parameters: 162419712\n",
      "weight_tying flag: False\n",
      "same object?: False\n",
      "out_head.weight shape: torch.Size([50257, 768])\n",
      "tok_emb.weight shape: torch.Size([50257, 768])\n",
      "check_flag is True; output of train_file\n",
      "/home/markb/llm-from-scratch/data/the-verdict.txt\n",
      "Characters: 20479\n",
      "Tokens: 5145\n",
      "\n",
      "\n",
      "Show train_loader first entry (converted to text):\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)  \"The height of his glory\"--that was what the women called it. I can hear Mrs. Gideon Thwing--his last Chicago sitter--deploring his unaccountable abdication. \"Of course it's going to send the value of my picture 'way up; but I don't think of that, Mr. Rickham--the loss to Arrt is all I think of.\" The word, on Mrs. Thwing's lips, multiplied its _rs_ as though they were reflected in an endless vista of mirrors. And it was not only the Mrs. Thwings who mourned. Had not the exquisite Hermia Croft, at the last Grafton Gallery show, stopped me before Gisburn's \"Moon-dancers\" to say, with tears in her eyes: \"We shall not look upon its like again\"?  Well!--\n",
      "Show val_loader first entry (converted to text):\n",
      "ue' collapsed like a house of cards. He didn't sneer, you understand, poor Stroud--he just lay there quietly watching, and on his lips, through the gray beard, I seemed to hear the question: 'Are you sure you know where you're coming out?'  \"If I could have painted that face, with that question on it, I should have done a great thing. The next greatest thing was to see that I couldn't--and that grace was given me. But, oh, at that minute, Rickham, was there anything on earth I wouldn't have given to have Stroud alive before me, and to hear him say: 'It's not too late--I'll show you how'?  \"It _was_ too late--it would have been, even if he'd been alive. I packed up my traps, and went down and told Mrs. Stroud. Of course I didn't tell her _that_--it would have been Greek to her. I simply said I couldn't paint him, that I was too moved. She rather liked the idea--she's so romantic! It was that that made her give me the donkey. But she was terribly upset at not getting the portrait--she\n",
      "tok_emb weight: mean, std: -3.4548206713225227e-06 0.020002231001853943\n",
      "out_head weight: mean, std: 7.737388045825355e-07 0.020000450313091278\n",
      "embeds std: 0.0200754813849926\n",
      "logits: mean, std, min, max: -0.00021281492081470788 0.5549576282501221 -2.9082236289978027 2.9592580795288086\n",
      "Ep 1 (Step 000000): Train loss 10.085, Val loss 10.153\n",
      "Ep 1 (Step 000005): Train loss 8.261, Val loss 8.454\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "Ep 2 (Step 000010): Train loss 6.973, Val loss 7.199\n",
      "Ep 2 (Step 000015): Train loss 6.292, Val loss 6.724\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "Ep 3 (Step 000020): Train loss 6.219, Val loss 6.678\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg2\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 63\u001b[39m, in \u001b[36mrun_training\u001b[39m\u001b[34m(cfg)\u001b[39m\n\u001b[32m     61\u001b[39m num_epochs = cfg[\u001b[33m'\u001b[39m\u001b[33mnum_epochs\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     62\u001b[39m optimizer = training_utils.setup_optimizer(model, cfg)\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m train_losses, val_losses, tokens_seen, global_step = \u001b[43mtraining_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_model_simple\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdevice_name\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnum_epochs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_context\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEvery effort moves you\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# plot and save plot\u001b[39;00m\n\u001b[32m     77\u001b[39m epochs_tensor = torch.linspace(\u001b[32m0\u001b[39m, cfg[\u001b[33m'\u001b[39m\u001b[33mnum_epochs\u001b[39m\u001b[33m'\u001b[39m], \u001b[38;5;28mlen\u001b[39m(train_losses))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/llm-from-scratch/src/llm_from_scratch/training/training_utils.py:170\u001b[39m, in \u001b[36mtrain_model_simple\u001b[39m\u001b[34m(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context, tokenizer)\u001b[39m\n\u001b[32m    166\u001b[39m optimizer.zero_grad()\n\u001b[32m    167\u001b[39m loss=calc_loss_batch(\n\u001b[32m    168\u001b[39m     input_batch, target_batch, model, device\n\u001b[32m    169\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m optimizer.step()\n\u001b[32m    172\u001b[39m tokens_seen += input_batch.numel()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/llmenv-first/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/llmenv-first/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/llmenv-first/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "run_training(cfg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c906f18b",
   "metadata": {},
   "source": [
    "## Experiment 1a: make sure that model really resets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7529c5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg2['run_name']=\"gpt2_basic_exp_1a\"\n",
    "run_training(cfg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b540d142",
   "metadata": {},
   "source": [
    "## Experiment 2: change stride to do more training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9752dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg2['run_name']=\"gpt2_basic_exp_2\"\n",
    "cfg2['stride']=128\n",
    "cfg2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3f3de1",
   "metadata": {},
   "source": [
    "### run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2039ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training(cfg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34355a1",
   "metadata": {},
   "source": [
    "## Experiment 3: try to do the weight-tying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ede47b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg2['run_name']=\"gpt2_basic_exp_3\"\n",
    "cfg2['model_config']['weight_tying']=True\n",
    "cfg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bf687e",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training(cfg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e32207",
   "metadata": {},
   "source": [
    "## Experiment 4: do more epochs, with weight tying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edfee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg2['run_name']=\"gpt2_basic_exp_4\"\n",
    "cfg2['model_config']['weight_tying']=True\n",
    "cfg2['num_epochs']=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e402ecaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training(cfg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f21e1f",
   "metadata": {},
   "source": [
    "## Experiment 5: mini-model, 2 layers, 2 heads per layer, with weight tying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e24b3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg2['run_name']=\"gpt2_basic_exp_5\"\n",
    "cfg2['model_config']['weight_tying']=True\n",
    "cfg2['num_epochs']=2\n",
    "cfg2['model_config']['n_layers']=2\n",
    "cfg2['model_config']['n_heads']=2\n",
    "cfg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a64d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training(cfg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec994124",
   "metadata": {},
   "source": [
    "## Experiment 6: mini-model, 2 layers, 2 heads per layer, emb_dim made small with weight tying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43e1728",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg2['run_name']=\"gpt2_basic_exp_6\"\n",
    "cfg2['model_config']['weight_tying']=True\n",
    "cfg2['num_epochs']=2\n",
    "cfg2['model_config']['n_layers']=2\n",
    "cfg2['model_config']['n_heads']=2\n",
    "cfg2['model_config']['emb_dim']=384\n",
    "cfg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed90a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training(cfg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e398512",
   "metadata": {},
   "source": [
    "## Experiment 7: mini-model, 2 layers, 2 heads per layer, emb_dim made small with weight tying, 8 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1589f49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg2['run_name']=\"gpt2_basic_exp_6\"\n",
    "cfg2['model_config']['weight_tying']=True\n",
    "cfg2['num_epochs']=8\n",
    "cfg2['model_config']['n_layers']=2\n",
    "cfg2['model_config']['n_heads']=2\n",
    "cfg2['model_config']['emb_dim']=384\n",
    "cfg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4556c22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training(cfg2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv-first",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
